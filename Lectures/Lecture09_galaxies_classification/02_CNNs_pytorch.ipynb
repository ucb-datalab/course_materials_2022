{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "02_CNNs-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RDMy90B6-PEy",
        "IXWdI2td-PE5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ImDuS8-PEa"
      },
      "source": [
        "%run ../talktools.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai72An86lqSS"
      },
      "source": [
        "# do this step below to get lightning, lightning bolts, etc.\n",
        "!pip install lightning-bolts torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kdonR3h-PEn"
      },
      "source": [
        "# Classification with pytorch Lightning\n",
        "\n",
        "*AY 128/256 (UC Berkeley, 2018-2021)*\n",
        "\n",
        "We saw last week how to do regression problems with neural nets in `keras`. Here we'll work with the pytorch equivalent, called pytorch lightning. Let's now explore classification, on images. Let's introduce the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist#labels) dataset: 70k small (28$\\times$28) images of 10 different types of clothing.\n",
        "\n",
        "<img src=\"https://github.com/zalandoresearch/fashion-mnist/blob/master/doc/img/fashion-mnist-sprite.png?raw=true\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJxs6AOJ-PEp"
      },
      "source": [
        "Each training and test example is assigned to one of the following labels:\n",
        "\n",
        "| Label | Description |\n",
        "| --- | --- |\n",
        "| 0 | T-shirt/top |\n",
        "| 1 | Trouser |\n",
        "| 2 | Pullover |\n",
        "| 3 | Dress |\n",
        "| 4 | Coat |\n",
        "| 5 | Sandal |\n",
        "| 6 | Shirt |\n",
        "| 7 | Sneaker |\n",
        "| 8 | Bag |\n",
        "| 9 | Ankle boot |\n",
        "\n",
        "Tensorflow has a simple method to get this data locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B6atmbVWAa7"
      },
      "source": [
        "def output_label(label):\n",
        "    output_mapping = {\n",
        "                 0: \"T-shirt/Top\",\n",
        "                 1: \"Trouser\",\n",
        "                 2: \"Pullover\",\n",
        "                 3: \"Dress\",\n",
        "                 4: \"Coat\", \n",
        "                 5: \"Sandal\", \n",
        "                 6: \"Shirt\",\n",
        "                 7: \"Sneaker\",\n",
        "                 8: \"Bag\",\n",
        "                 9: \"Ankle Boot\"\n",
        "                 }\n",
        "    input = (label.item() if type(label) == torch.Tensor else label)\n",
        "    return output_mapping[input]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o566vXl-PEq"
      },
      "source": [
        "import datetime, os\n",
        "import numpy as np\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.external import mathjax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.functional import accuracy\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# use a GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"pytorch version:\", torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCwBscaPiTtv"
      },
      "source": [
        "Let's get the training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgaGA_kBiTtv"
      },
      "source": [
        "%%bash\n",
        "wget \"https://github.com/fpleoni/fashion_mnist/blob/master/fashion-mnist_train.csv?raw=true\" --output-document=fashion-mnist_train.csv\n",
        "wget \"https://github.com/fpleoni/fashion_mnist/blob/master/fashion-mnist_test.csv?raw=true\" --output-document=fashion-mnist_test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG5avg7hiTtx"
      },
      "source": [
        "train_csv = pd.read_csv(\"./fashion-mnist_train.csv\")\n",
        "test_csv = pd.read_csv(\"./fashion-mnist_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOL83YFQiTty"
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "class FashionDataset(Dataset):\n",
        "    \"\"\"User defined class to build a datset using Pytorch class Dataset.\"\"\"\n",
        "    \n",
        "    def __init__(self, data, transform = None):\n",
        "        \"\"\"Method to initilaize variables.\"\"\" \n",
        "        self.fashion_MNIST = list(data.values)\n",
        "        self.transform = transform\n",
        "        \n",
        "        label = []\n",
        "        image = []\n",
        "        \n",
        "        for i in self.fashion_MNIST:\n",
        "             # first column is of labels.\n",
        "            label.append(i[0])\n",
        "            image.append(i[1:])\n",
        "        self.labels = np.asarray(label)\n",
        "        # Dimension of Images = 28 * 28 * 1. where height = width = 28 and color_channels = 1.\n",
        "        self.images = np.asarray(image).reshape(-1, 28, 28, 1).astype('float32')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        image = self.images[index]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "# Transform data into Tensor that has a range from 0 to 1\n",
        "train_set = FashionDataset(train_csv, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "test_set = FashionDataset(test_csv, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=2)\n",
        "test_loader = DataLoader(train_set, batch_size=batch_size, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmNypYL0-PEr"
      },
      "source": [
        "a = next(iter(train_loader))\n",
        "a[0].size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb-24iwm-PEr"
      },
      "source": [
        "len(train_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhfigpH-PEu"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image, label = next(iter(train_set))\n",
        "plt.axis('off')\n",
        "\n",
        "plt.imshow(image.squeeze(), cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "output_label(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RybvUvM_-PEv"
      },
      "source": [
        "To learn a model to predict the class of a given image, we could treat this 28$\\times$28 image as 1d input, like a stellar spectrum:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq5ew8nh-PEv"
      },
      "source": [
        "ind=20\n",
        "_ = plt.plot(a[0][ind].numpy().reshape(-1))\n",
        "plt.ylabel(\"normalized flux\")\n",
        "plt.xlabel(\"1D pixel index\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wyfn1k4-PEw"
      },
      "source": [
        "But this *clearly* jumbles the inherent spatial structure and local correlations found in natural images. Using just Dense layers in a NN we'd effectively be asking the network to learn these correlations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkv4gLbc-PEx"
      },
      "source": [
        "## Convolutional Neural Nets (ConvNets)\n",
        "\n",
        "NNs built for images (or more generally, inputs with spatial structure).\n",
        "\n",
        "### Key Ideas: \n",
        "  - layers see only parts of each image (effectively all other weights are zero).\n",
        "  - some layers do simple operations on previous layers to reduce dimensionality (e.g., take the largest value in a a 3x3 range)\n",
        "  - \"Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.\"\n",
        " \n",
        "<img src=\"http://cs231n.github.io/assets/cnn/cnn.jpeg\">\n",
        "\n",
        "<img src=\"http://cs231n.github.io/assets/cnn/depthcol.jpeg\">\n",
        "\n",
        "\"An example input volume in red (e.g. a 32x32x3 CIFAR-10 image), and an example volume of neurons in the first Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (5 in this example) along the depth, all looking at the same region in the input - see discussion of depth columns in text below. \"\n",
        "\n",
        "cf. http://cs231n.github.io/convolutional-networks/\n",
        "\n",
        "<img src=\"data/f2.png\">\n",
        "Source: http://www.nature.com/nature/journal/v521/n7553/fig_tab/nature14539_F2.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDMy90B6-PEy"
      },
      "source": [
        "### Filter banks\n",
        "\n",
        "  http://setosa.io/ev/image-kernels/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml0bs_8U-PEy"
      },
      "source": [
        "### Pooling\n",
        "\n",
        "<img src=\"http://cs231n.github.io/assets/cnn/pool.jpeg\" width=\"40%\">\n",
        "<img src=\"http://cs231n.github.io/assets/cnn/maxpool.jpeg\" width=\"40%\">\n",
        "Source: http://cs231n.github.io/convolutional-networks/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPkNj3uh-PEz"
      },
      "source": [
        "nb_classes = 10\n",
        "\n",
        "demo_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
        "\n",
        "batch = next(iter(demo_loader))\n",
        "images, labels = batch\n",
        "print(type(images), type(labels))\n",
        "print(images.shape, labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WPJO8Jw-PEz"
      },
      "source": [
        "y = torch.nn.functional.one_hot(labels)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHd9NXJ9-PEz"
      },
      "source": [
        "We want our output predictions to look like a \"probability\" of belonging to one of the 10 classes. And, importantly, we'd like to make sure that the probability over all classes sums to unity. One way to do this is to scale the outputs of the last layer using a [`softmax`](https://en.wikipedia.org/wiki/Softmax_function):\n",
        "\n",
        "$$\n",
        "{\\rm softmax}(\\vec s) = \\frac{e^{s_i}}{\\sum_i e^{s_i}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTPwpm9t-PE0"
      },
      "source": [
        "So if the (unnormalized) prediction from am NN for an image is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qSvdX5-PE0"
      },
      "source": [
        "s = np.random.normal(size=(10,))\n",
        "s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb9kUxsZ-PE0"
      },
      "source": [
        "Then the softmax scaling gives us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9uCUoUa-PE1"
      },
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x)/np.sum(np.exp(x))\n",
        "\n",
        "print(softmax(s))\n",
        "np.testing.assert_almost_equal(softmax(s).sum(), 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QmXQ7xERB09"
      },
      "source": [
        "from scipy.special import softmax as sp_softmax\n",
        "sp_softmax(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZIiiTry-PE1"
      },
      "source": [
        "We'll use the \"categorical cross-entropy\" loss:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS3Sopro-PE2"
      },
      "source": [
        "<img src=\"https://gombru.github.io/assets/cross_entropy_loss/softmax_CE_pipeline.png\">\n",
        "Source: https://gombru.github.io/2018/05/23/cross_entropy_loss/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P7vAxRG-PE2"
      },
      "source": [
        "# perfect match ... use a small ϵ to avoid taking log(0) since lim x log x -> 0 as x->0\n",
        "print(\"loss with a perfect match:\", -(y[0].double() @ np.log(y[0].double() + 1e-16)).numpy())\n",
        "print(\"loss with a predicted match:\", -(y[0].double() @ np.log(softmax(s) + 1e-16)).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRlyeM7J-PE3"
      },
      "source": [
        "## Building a CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHGC36yoiTt8"
      },
      "source": [
        "import pytorch_lightning as pl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83LYcCd2iTt8"
      },
      "source": [
        "class mycnn(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # set this to an example input size to the see a summary\n",
        "        # see https://pytorch-lightning.readthedocs.io/en/latest/common/debugging.html\n",
        "        self._example_input_array = torch.randn((1, 1, 28, 28))\n",
        "\n",
        "        # define the layers here\n",
        "        # Conv2d(in_channels, out_channels, kernel_size)\n",
        "        # see https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3),\n",
        "            \n",
        "            # see https://github.com/sksq96/pytorch-summary/issues/55#issuecomment-471844028\n",
        "            # to understand why pytorch and keras differ here\n",
        "            nn.BatchNorm2d(32, affine=False),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        \n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        \n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        self.fc1=torch.nn.Linear(1152, 128)\n",
        "        self.fc2=torch.nn.Linear(128, 32)\n",
        "        self.fc3=torch.nn.Linear(32, 10)\n",
        "    \n",
        "        self.loss = nn.NLLLoss()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x=torch.relu(self.fc1(x))\n",
        "        x=torch.relu(self.fc2(x))\n",
        "        x=F.log_softmax(self.fc3(x), dim=-1)\n",
        "        return x\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters())\n",
        "        \n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',\n",
        "            factor=0.75,\n",
        "            patience=2,\n",
        "            min_lr=1e-6,\n",
        "            verbose=True\n",
        "        )\n",
        "        \n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_accuracy\"}\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.loss(logits, y)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "    \n",
        "    def _evaluate(self, batch, batch_idx, stage=None):\n",
        "        x, y = batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.loss(logits, y)\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        acc = accuracy(preds, y)\n",
        "\n",
        "        if stage:\n",
        "            self.log(f'{stage}_loss', loss, prog_bar=True)\n",
        "            self.log(f'{stage}_accuracy', acc, prog_bar=True)\n",
        "\n",
        "        return loss, acc\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self._evaluate(batch, batch_idx, 'val')[0]\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return train_loader\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukdCwzPRiTt9"
      },
      "source": [
        "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
        "filename = f'datalab_nn_pytorch_{run_time_string}'\n",
        "\n",
        "early_stop_callback = EarlyStopping(\n",
        "   monitor='val_accuracy',\n",
        "   min_delta=0.001,\n",
        "   patience=3,\n",
        "   verbose=True,\n",
        "   mode='max'\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    dirpath='nn_results',\n",
        "    filename=filename,\n",
        "    verbose=True,\n",
        "    save_top_k=1\n",
        ")\n",
        "\n",
        "logger = [CSVLogger(\"nn_results1\", name=filename), TensorBoardLogger(\"nn_results\", name=filename)]\n",
        "\n",
        "pl.seed_everything(42)\n",
        "\n",
        "myTrainer=pl.Trainer(callbacks=[early_stop_callback, checkpoint_callback], logger=logger,\n",
        "                     gpus=-1, accelerator='dp', auto_select_gpus=True, max_epochs=20)\n",
        "model=mycnn()\n",
        "myTrainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXWdI2td-PE5"
      },
      "source": [
        "### Aside: Dropout \n",
        "\n",
        "You'll notice above that the `accuracy` is much higher than the `val_accuracy`. That is, we overfit on the training data. One way to help protect against this is to introduce `Dropout`\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*iWQzxhVlvadk6VAJjsgXgg.png\">\n",
        "\n",
        "Srivastava, Nitish, et al. ”Dropout: a simple way to prevent neural networks from\n",
        "overfitting”, JMLR 2014\n",
        "\n",
        "```python\n",
        "        x = self.layer3(x)\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1) \n",
        "        x = nn.Dropout(p=0.1)(x) # 10% of dropping an output connection\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeIpKHxB-PE5"
      },
      "source": [
        "### Aside: Visualization of the layers\n",
        "\n",
        "From François Chollet (“DEEP LEARNING with Python”):\n",
        "\n",
        "Intermediate activations are “useful for understanding how successive convnet layers transform their input, and for getting a first idea of the meaning of individual convnet filters.”\n",
        "\n",
        "“The representations learned by convnets are highly amenable to visualization, in large part because they’re representations of visual concepts. Visualizing intermediate activations consists of displaying the feature maps that are output by various convolution and pooling layers in a network, given a certain input (the output of a layer is often called its activation, the output of the activation function). This gives a view into how an input is decomposed into the different filters learned by the network. Each channel encodes relatively independent features, so the proper way to visualize these feature maps is by independently plotting the contents of every channel as a 2D image.”\n",
        "\n",
        "Following from https://github.com/gabrielpierobon/cnnshapes/blob/master/README.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD8Ajblof7bp"
      },
      "source": [
        "# Visualize feature maps\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "data, _ = next(iter(train_set))\n",
        "data.unsqueeze_(0)\n",
        "\n",
        "for layer_name, layer in [('layer1', model.layer1), \n",
        "                          ('layer2', model.layer2), ('layer3', model.layer3)]:\n",
        "    layer.register_forward_hook(get_activation(layer_name))\n",
        "    output = model(data)\n",
        "\n",
        "    layer_activation = activation[layer_name].squeeze()\n",
        "\n",
        "    images_per_row = 15\n",
        "    n_features = layer_activation.shape[0]   # Number of features in the feature map\n",
        "    size = layer_activation.shape[1] # The feature map has shape (n_features, size, size).\n",
        "    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[col * images_per_row + row,\n",
        "                                              :, :]\n",
        "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
        "            channel_image /= channel_image.std()\n",
        "            channel_image *= 64\n",
        "            channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255)\n",
        "            display_grid[col * size : (col + 1) * size, # Displays the grid\n",
        "                          row * size : (row + 1) * size] = channel_image\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcFKFine-PE6"
      },
      "source": [
        "# Data Augmentation\n",
        "\n",
        "Another way to avoid overfitting, aside from `Dropout`, is to increase the number of exmaples used to to train the model.  Data augmentation is a generic term for methods used to expand the effect training set size by generating more data from the original training set. In images, this is pretty natural: scale changes, rotations, flips, etc. should still give us the same label. This method has the benefit of usually increasing test-time accuracy.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1200/1*C8hNiOqur4OJyEZmC7OnzQ.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyrkit-P-PE7"
      },
      "source": [
        "In Pytorch see https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html and https://pytorch.org/vision/stable/transforms.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbxdGLblmeKe"
      },
      "source": [
        "train_transforms = transforms.Compose([\n",
        "        torchvision.transforms.ToPILImage(),\n",
        "        torchvision.transforms.RandomAffine(degrees=15, shear=0.1),\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_set = FashionDataset(train_csv, transform=train_transforms)\n",
        "test_set = FashionDataset(test_csv, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=2)\n",
        "test_loader = DataLoader(train_set, batch_size=batch_size, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNQ6SWYwmjqX"
      },
      "source": [
        "image, label = next(iter(train_set))\n",
        "plt.axis('off')\n",
        "\n",
        "plt.imshow(image.squeeze(), cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "output_label(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftp5zXGFpi29"
      },
      "source": [
        "class mycnn_dropout(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # set this to an example input size to the see a summary\n",
        "        # see https://pytorch-lightning.readthedocs.io/en/latest/common/debugging.html\n",
        "        self._example_input_array = torch.randn((1, 1, 28, 28))\n",
        "\n",
        "        # define the layers here\n",
        "        # Conv2d(in_channels, out_channels, kernel_size)\n",
        "        # see https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3),\n",
        "            \n",
        "            # see https://github.com/sksq96/pytorch-summary/issues/55#issuecomment-471844028\n",
        "            # to understand why pytorch and keras differ here\n",
        "            nn.BatchNorm2d(32, affine=False),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        \n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(p=0.1)\n",
        "        )\n",
        "        \n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        self.fc1=torch.nn.Linear(1152, 128)\n",
        "        self.fc2=torch.nn.Linear(128, 32)\n",
        "        self.fc3=torch.nn.Linear(32, 10)\n",
        "    \n",
        "        self.loss = nn.NLLLoss()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # add dropout \n",
        "        x = nn.Dropout(p=0.2)(x)\n",
        "\n",
        "        x=torch.relu(self.fc1(x))\n",
        "        x=torch.relu(self.fc2(x))\n",
        "        x=F.log_softmax(self.fc3(x), dim=-1)\n",
        "        return x\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters())\n",
        "        \n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',\n",
        "            factor=0.75,\n",
        "            patience=2,\n",
        "            min_lr=1e-6,\n",
        "            verbose=True\n",
        "        )\n",
        "        \n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_accuracy\"}\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.loss(logits, y)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "    \n",
        "    def _evaluate(self, batch, batch_idx, stage=None):\n",
        "        x, y = batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.loss(logits, y)\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        acc = accuracy(preds, y)\n",
        "\n",
        "        if stage:\n",
        "            self.log(f'{stage}_loss', loss, prog_bar=True)\n",
        "            self.log(f'{stage}_accuracy', acc, prog_bar=True)\n",
        "\n",
        "        return loss, acc\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self._evaluate(batch, batch_idx, 'val')[0]\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return train_loader\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTooZRBQqHZq"
      },
      "source": [
        "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
        "filename = f'datalab_nn_pytorch_dropout_{run_time_string}'\n",
        "\n",
        "early_stop_callback = EarlyStopping(\n",
        "   monitor='val_accuracy',\n",
        "   min_delta=0.001,\n",
        "   patience=3,\n",
        "   verbose=True,\n",
        "   mode='max'\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    dirpath='nn_results',\n",
        "    filename=filename,\n",
        "    verbose=True,\n",
        "    save_top_k=1\n",
        ")\n",
        "\n",
        "logger = [CSVLogger(\"nn_results1\", name=filename), TensorBoardLogger(\"nn_results\", name=filename)]\n",
        "\n",
        "pl.seed_everything(42)\n",
        "\n",
        "myTrainer=pl.Trainer(callbacks=[early_stop_callback, checkpoint_callback], logger=logger,\n",
        "                     gpus=-1, accelerator='dp', auto_select_gpus=True, max_epochs=20)\n",
        "model_dropout=mycnn_dropout()\n",
        "myTrainer.fit(model_dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlHcTlnzrJ10"
      },
      "source": [
        "ls -t1 nn_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkSgsL7j-PFH"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "latest_log_file = !ls -t1 nn_results1/{filename}/version_*/metrics.csv | head -1\n",
        "latest_model_file = !ls -t1 nn_results/{filename}.ckpt | head -1\n",
        "\n",
        "hist_df = pd.read_csv(latest_log_file[0])\n",
        "hist_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHvNZWnBwaA0"
      },
      "source": [
        "latest_model_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2Nezvpn-PFI"
      },
      "source": [
        "# reload the best model\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "\n",
        "model = load_model(latest_model_file[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCd-gIrvwfvu"
      },
      "source": [
        "model_dropout.eval()  # set model to evaluation mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDrzoupJw8tn"
      },
      "source": [
        "image, label = next(iter(test_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoBYga5IwumJ"
      },
      "source": [
        "prediction = (np.e**model_dropout(image.unsqueeze(0))).detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMfBliArUTg3"
      },
      "source": [
        "prediction.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuO0oM3AUXnR"
      },
      "source": [
        "np.argmax(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4LKeEARUeOd"
      },
      "source": [
        "plt.axis('off')\n",
        "_ = plt.imshow(image[0,:,:], cmap=plt.cm.gray_r, interpolation='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jnVnjA6U1e-"
      },
      "source": [
        "print(output_label(np.argmax(prediction)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBJPKYc-Zfcy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}